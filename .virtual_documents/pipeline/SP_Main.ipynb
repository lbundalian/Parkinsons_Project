








import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import StandardScaler, MinMaxScaler
from sklearn.model_selection import train_test_split
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import classification_report, accuracy_score
from sklearn.experimental import enable_iterative_imputer
from sklearn.impute import IterativeImputer
from imblearn.over_sampling import SMOTE
from collections import Counter
import warnings


# Import functions from our files
from modules import *


# Load the Excel file into a DataFrame
data_file = 'data/parkinsons_data.csv'
df = pd.read_csv(data_file)





# Dictionary of names
dict_names = {
    'name':'Name',
    'MDVP:Fo(Hz)': 'MDVP_Fo',
    'MDVP:Fhi(Hz)': 'MDVP_Fhi',
    'MDVP:Flo(Hz)': 'MDVP_Flo',
    'MDVP:Jitter(%)': 'MDVP_Jitter_percent',
    'MDVP:Jitter(Abs)': 'MDVP_Jitter_Abs',
    'MDVP:RAP': 'MDVP_RAP',
    'MDVP:PPQ': 'MDVP_PPQ',
    'Jitter:DDP': 'Jitter_DDP',
    'MDVP:Shimmer': 'MDVP_Shim',
    'MDVP:Shimmer(dB)': 'MDVP_Shim_dB',
    'Shimmer:APQ3': 'Shimmer_APQ3',
    'Shimmer:APQ5': 'Shimmer_APQ5',
    'MDVP:APQ': 'MDVP_APQ',
    'Shimmer:DDA': 'Shimmer_DDA',
    'NHR': 'NHR',
    'HNR': 'HNR',
    'status': 'Status',
    'RPDE': 'RPDE',
    'DFA': 'DFA',
    'spread1': 'Spread1',
    'spread2': 'Spread2',
    'D2': 'D2',
    'PPE': 'PPE'
}

# Apply the function to rename the columns
renamed_df = rename_variables(df, dict_names)

# View the result
print(renamed_df.columns)





count_nulls(renamed_df)





# Visualize outliers
generate_boxplots(renamed_df, None)


# Handle outliers
df_no_outliers = handle_outliers(renamed_df)


df_no_outliers.head()





df_no_corr, corr_vbles = drop_correlated_vbles(df_no_outliers, 0.75)


# This is the new Data Frame, with no correlated variables
df_no_corr.head()


# This are the correlated variables that were removed
corr_vbles





# Define grouping variable
group_column = "Status"

# Select numeric columns
numeric_columns = df_no_corr.select_dtypes(include=['number']).columns

# Plot variables using the scatter_plot function
for i, var1 in enumerate(numeric_columns):
    for j, var2 in enumerate(numeric_columns):
        if i < j:  # Avoid duplicate pairs and self-pairs
            scatter_plot(df_no_corr, var1, var2, group_column)





# Show the summary of the data before and after
print("\nData summary before (original):\n")
print(renamed_df.describe())
print("\nData summary clean (no outliers or correlated variables):\n")
print(df_no_corr.describe())





grouped_df = group_and_average(df_no_corr)


grouped_df.head()





# Visualize the difference in the behavior of variables between patients and controls
generate_boxplots(grouped_df, 'Status')





# Apply the normalization function
normalized_df = normalize_dataframe(grouped_df, method='z-score', exclude_columns=['Subject_ID', 'Status'])


normalized_df.head()








# Divide the data into the appropiate X and y for training and testing
X_train_n, X_test_n, y_train_n, y_test_n = get_knn_data(normalized_df)


# Resample subsets to achieve a more balanced result
X_train_n_resamp, y_train_n_resamp = resample(X_train_n, y_train_n)





# Divide the data into the appropiate X and y for training and testing
X_train, X_test, y_train, y_test = get_knn_data(grouped_df)


# Resample subsets to achieve a more balanced result
X_train_resamp, y_train_resamp = resample(X_train, y_train)





# Create a list with possible n values (neighbors)
n_values = list([3, 5, 7, 9, 11, 15, 17, 19, 21])

# Apply the function to graph the accuracy against different values of n
find_best_n(n_values, X_train_n_resamp, y_train_n_resamp)

# The same n was taken for both models





# Choose a n = 5, since it's the highest accuracy that stabilizes for a series of n values
n = 5





# Create an instance of the ParkinsonsClassifier class
classifier_n = ParkinsonsClassifier()

# Train model
knn_model_n = classifier_n.train(X=X_train_n_resamp, y=y_train_n_resamp, k=n)


classifier_n.save_model("model_normalized.pkl")





# Create an instance of the ParkinsonsClassifier class
classifier = ParkinsonsClassifier()

# Train model
knn_model = classifier.train(X=X_train_resamp, y=y_train_resamp, k=n)


classifier.save_model("model_non-normalized.pkl")











predictions_n = classifier_n.predict(X_test_n)
predictions_n





predictions = classifier.predict(X_test)
predictions








# Check the performance of the current model
validator_n = CrossValidator(KNeighborsClassifier(n_neighbors=5), cv=5)


performance_metrics_n = validator_n.calculate_performance(predictions_n, y_test_n)
print(performance_metrics_n)


validation_metrics_n = validator_n.validate(X_test_n, y_test_n)


def plot_confusion_matrix(conf_matrix):

    plt.figure(figsize=(8, 6))
    sns.heatmap(conf_matrix, annot=True, fmt="d", cmap="Blues")
    plt.xlabel("Predicted Labels")
    plt.ylabel("True Labels")
    plt.title("Confusion Matrix")
    plt.show()


plot_confusion_matrix(performance_metrics_n['confusion_matrix'])


plot_confusion_matrix(validation_metrics_n['confusion_matrix'])





# Check the performance of the current model
validator = CrossValidator(KNeighborsClassifier(n_neighbors=5), cv=5)
performance_metrics = validator.calculate_performance(predictions,y_test)
print(performance_metrics)


validation_metrics = validator.validate(X_test,y_test)


def plot_confusion_matrix(conf_matrix):

    plt.figure(figsize=(8, 6))
    sns.heatmap(conf_matrix, annot=True, fmt="d", cmap="Blues")
    plt.xlabel("Predicted Labels")
    plt.ylabel("True Labels")
    plt.title("Confusion Matrix")
    plt.show()


plot_confusion_matrix(performance_metrics['confusion_matrix'])


plot_confusion_matrix(validation_metrics['confusion_matrix'])


classifier.save_model("model.pkl")






