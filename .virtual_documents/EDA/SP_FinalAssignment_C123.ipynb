import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns


# Load the Excel file into a DataFrame
data_file = 'data/parkinsons_data.csv'  # Replace with the path to your file
df = pd.read_csv(data_file)





def rename_variables(df, dict_names):
    """
    Rename variables in a given dataframe.

    Parameters:
        df (pd.DataFrame): Original dataframe
        dict_names (dict): Dictionary mapping current column names to new names

    Returns:
        renamed_df (pd.DataFrame): Dataframe with renamed columns
    """
    renamed_df = df.rename(columns=dict_names)
    return renamed_df



# Dictionary of names
dict_names = {
    'name':'Name',
    'MDVP:Fo(Hz)': 'MDVP_Fo',
    'MDVP:Fhi(Hz)': 'MDVP_Fhi',
    'MDVP:Flo(Hz)': 'MDVP_Flo',
    'MDVP:Jitter(%)': 'MDVP_Jitter_percent',
    'MDVP:Jitter(Abs)': 'MDVP_Jitter_Abs',
    'MDVP:RAP': 'MDVP_RAP',
    'MDVP:PPQ': 'MDVP_PPQ',
    'Jitter:DDP': 'Jitter_DDP',
    'MDVP:Shimmer': 'MDVP_Shim',
    'MDVP:Shimmer(dB)': 'MDVP_Shim_dB',
    'Shimmer:APQ3': 'Shimmer_APQ3',
    'Shimmer:APQ5': 'Shimmer_APQ5',
    'MDVP:APQ': 'MDVP_APQ',
    'Shimmer:DDA': 'Shimmer_DDA',
    'NHR': 'NHR',
    'HNR': 'HNR',
    'status': 'Status',
    'RPDE': 'RPDE',
    'DFA': 'DFA',
    'spread1': 'Spread1',
    'spread2': 'Spread2',
    'D2': 'D2',
    'PPE': 'PPE'
}

# Apply the function to rename the columns
renamed_df = rename_variables(df, dict_names)

# View the result
print(renamed_df.columns)







# Calculate and display the number of null values in the dataset
null_counts = renamed_df.isnull().sum()
print("\nNumber of null values in the dataset:\n", null_counts)





# Define the name of the status column (replace it with the actual name of your status column)
status_column = 'Status'  # Change 'Exitus' to the actual name of your status column

# Get the numerical columns (excluding the status column)
numeric_columns = renamed_df.select_dtypes(include=['float64', 'int64']).columns

# Exclude the status column from the numerical columns
numeric_columns = [col for col in numeric_columns if col != status_column]

# Create a boxplot for each numerical column to visualize the outliers before modifying them
for col in numeric_columns:
    plt.figure(figsize=(8, 6))
    sns.boxplot(data=renamed_df, x=col, color='skyblue')
    plt.title(f"Boxplot of '{col}' (Before replacing outliers)")
    plt.xlabel(col)
    plt.ylabel("Values")
    plt.show()

# Calculate the quartiles (Q1, Q3) for the remaining numerical columns
Q1 = renamed_df[numeric_columns].quantile(0.25)
Q3 = renamed_df[numeric_columns].quantile(0.75)

# Calculate the interquartile range (IQR)
IQR = Q3 - Q1

# Identify the outliers
outliers = ((renamed_df[numeric_columns] < (Q1 - 1.5 * IQR)) | (renamed_df[numeric_columns] > (Q3 + 1.5 * IQR)))

# Display the outliers identified by variable
print("\nOutliers identified by variable:")

# Replace the outliers with NaN
renamed_df[outliers] = np.nan

# Display how many outliers were replaced
for col in numeric_columns:
    print(f"{col}: {outliers[col].sum()} values replaced with NaN")

# Display the rows with outliers before marking them as NaN
outliers_rows = renamed_df[outliers.any(axis=1)]
print("\nRows with outliers marked as NaN:")
print(outliers_rows)



# Calculate and display the number of null values in the dataset
null_counts = renamed_df.isnull().sum()
print("\nNumber of null values in the dataset:\n", null_counts)


# Create a boxplot for each numerical column to visualize outliers
for col in numeric_columns:
    plt.figure(figsize=(8, 6))
    sns.boxplot(data=renamed_df, x=col, color='skyblue')
    plt.title(f"Boxplot of '{col}'(After replacing outliers)")
    plt.xlabel(col)
    plt.ylabel("Values")
    plt.show()



from sklearn.experimental import enable_iterative_imputer
from sklearn.impute import IterativeImputer

# Define the status column (replace with the actual name of your status column)
status_column = 'Status'  # Change 'Exitus' to the actual name of your status column

# Get the numerical columns (excluding the status column)
numeric_columns = renamed_df.select_dtypes(include=['float64', 'int64']).columns

# Exclude the status column from the numerical columns
numeric_columns = [col for col in numeric_columns if col != status_column]

# Apply the Iterative Imputer to the numerical columns with missing values
iterative_imputer = IterativeImputer(max_iter=10, random_state=42)  # You can set max_iter and random_state as needed

# Fit and transform the numeric columns with missing values
renamed_df[numeric_columns] = iterative_imputer.fit_transform(renamed_df[numeric_columns])

# Display the dataframe after imputation
print("\nDataFrame after applying Iterative Imputer:")
print(renamed_df.head())  # Print the first few rows to check the imputed values


# Calculate and display the number of null values in the dataset
null_counts = renamed_df.isnull().sum()
print("\nNumber of null values in the dataset:\n", null_counts)





# Summary of the data (statistical description)
summary = renamed_df.describe()

renamed_df['Status'] = renamed_df['Status'].astype(int)  # Convert to integers


# Filter the numerical columns in the dataframe
numeric_columns = renamed_df.select_dtypes(include=['float64']).columns


# Calculate the correlation matrix only with the numerical columns
correlation_matrix = renamed_df[numeric_columns].corr()

# Create a heatmap to visualize the correlation matrix
plt.figure(figsize=(12, 8))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt='.2f', linewidths=0.5)
plt.title('Correlation Matrix')
plt.show()



# Identify highly correlated variables and remove them
highly_correlated_vars = set()
threshold = 0.75
target_variable = 'Status'  # Change this to your target variable

for i in range(len(correlation_matrix.columns)):
    for j in range(i):
        if abs(correlation_matrix.iloc[i, j]) > threshold:
            var1 = correlation_matrix.columns[i]
            var2 = correlation_matrix.columns[j]
            corr_with_target_var1 = abs(renamed_df[var1].corr(renamed_df[target_variable]))
            corr_with_target_var2 = abs(renamed_df[var2].corr(renamed_df[target_variable]))
            if corr_with_target_var1 < corr_with_target_var2:
                highly_correlated_vars.add(var1)
            else:
                highly_correlated_vars.add(var2)

# Display the variables identified as highly correlated
print("\nHighly correlated variables to remove:", highly_correlated_vars)

# Drop the highly correlated variables
filtered_df = renamed_df.drop(columns=highly_correlated_vars)



filtered_df





def generate_boxplots(df, group_variable):
    """
    Generates boxplots for all numerical variables in the DataFrame based on the group variable (e.g., 'Status').

    Args:
    - df: Pandas DataFrame containing the data.
    - group_variable: Name of the variable to group by (e.g., 'Status').
    """
    # Filter numerical columns from the DataFrame
    numerical_columns = df.select_dtypes(include=['number']).columns

    # Create a plot for each numerical column
    for col in numerical_columns:
        if col != group_variable:
            plt.figure(figsize=(8, 6))
            df.boxplot(column=col, by=group_variable)
            plt.title(f'Boxplot of {col} by {group_variable}')
            plt.suptitle('')  # Suppress the default title
            plt.show()

# Using the function
generate_boxplots(filtered_df, 'Status')





def group_and_average(df):
    """
    Aggregate variables by averaging them for each subject.

    The subject and trial information is extracted from the 'Subject_ID' column. The output
    will have one row per subject, averaged across all trials.

    Parameters:
        df (pd.DataFrame): Dataframe containing the 'Subject_ID' column.

    Returns:
        aggregated_df (pd.DataFrame): Dataframe aggregated by subject.
    """
    # Extraer el identificador del sujeto
    df['name'] = df['Name'].str.extract(r'_S(\d+)_')[0]

    # Seleccionar solo columnas numÃ©ricas para evitar errores
    numeric_columns = df.select_dtypes(include=['float64', 'int64'])
    numeric_columns['name'] = df['name']  # Incluir la columna para agrupar

    # Agrupar por el nuevo identificador y calcular la media
    grouped = numeric_columns.groupby('name').mean().reset_index()

    return grouped


# Apply the function to the data
final_df = group_and_average(filtered_df)

# View the results
final_df.head()






# Create 'subject_id' column by extracting the subject part (e.g., S01, S50)
renamed_df['Subject_ID'] = renamed_df['Name'].str.extract(r'_(S\d+)_')[0]

# Create 'trial' column by extracting the trial number and prepending 't' (e.g., t1, t2)
renamed_df['Trial'] = 't' + renamed_df['Name'].str.extract(r'_(\d+)$')[0]

# Drop the 'name' column
renamed_df.drop(columns=['Name'], inplace=True)

# Display the modified DataFrame
renamed_df.head()





def scatter_plot(df, var1, var2, groups):
    """
    Create a scatter plot of two variables colored by group.

    Parameters:
        df (pd.DataFrame): Dataframe
        var1 (str): Variable for x-axis
        var2 (str): Variable for y-axis
        groups (str): Column defining groups
    """
    unique_groups = df[groups].unique()
    for group in unique_groups:
        subset = df[df[groups] == group]
        plt.scatter(subset[var1], subset[var2], label=group)

    plt.xlabel(var1)
    plt.ylabel(var2)
    plt.legend()
    plt.title(f'Scatterplot of {var1} vs {var2} by {groups}')
    plt.show()


scatter_plot(final_df, var1='MDVP_Fo', var2='MDVP_Fhi', groups='Status')






import pandas as pd
from sklearn.preprocessing import StandardScaler, MinMaxScaler

def normalize_dataframe(df, method='z-score', exclude_columns=None):
    """
    Normalizes all variables in the DataFrame using either Z-score or Min-Max normalization.

    Parameters:
    - df (pd.DataFrame): The input DataFrame.
    - method (str): The normalization method, either 'z-score' or 'min-max'.
    - exclude_columns (list): List of columns to exclude from normalization (e.g., identifiers, categorical variables).

    Returns:
    - pd.DataFrame: A DataFrame with normalized variables.
    """
    # Copy the dataframe to avoid modifying the original
    df_normalized = df.copy()

    # Exclude specific columns from normalization
    if exclude_columns is None:
        exclude_columns = []

    # Select columns to normalize (numerical columns excluding the excluded ones)
    columns_to_normalize = df.select_dtypes(include=['float64', 'int64']).columns.difference(exclude_columns)

    # Apply normalization
    if method == 'z-score':
        scaler = StandardScaler()
        df_normalized[columns_to_normalize] = scaler.fit_transform(df[columns_to_normalize])
    elif method == 'min-max':
        scaler = MinMaxScaler()
        df_normalized[columns_to_normalize] = scaler.fit_transform(df[columns_to_normalize])
    else:
        raise ValueError("Invalid normalization method. Choose 'z-score' or 'min-max'.")

    return df_normalized

# Example usage with your dataset:
# Assuming `df` is your DataFrame and excluding the "Name" and "Status" columns from normalization
normalized_df = normalize_dataframe(df, method='z-score', exclude_columns=['Name', 'Status', 'name'])
print(normalized_df.head())



import pandas as pd
from sklearn.preprocessing import MinMaxScaler

def normalize_dataframe(df, method='min-max', exclude_columns=None):
    """
    Normalizes all variables in the DataFrame using either Z-score or Min-Max normalization.

    Parameters:
    - df (pd.DataFrame): The input DataFrame.
    - method (str): The normalization method, either 'z-score' or 'min-max'.
    - exclude_columns (list): List of columns to exclude from normalization (e.g., identifiers, categorical variables).

    Returns:
    - pd.DataFrame: A DataFrame with normalized variables.
    """
    # Copy the dataframe to avoid modifying the original
    df_normalized = df.copy()

    # Exclude specific columns from normalization
    if exclude_columns is None:
        exclude_columns = []

    # Select columns to normalize (numerical columns excluding the excluded ones)
    columns_to_normalize = df.select_dtypes(include=['float64', 'int64']).columns.difference(exclude_columns)

    # Apply normalization
    if method == 'z-score':
        scaler = StandardScaler()
        df_normalized[columns_to_normalize] = scaler.fit_transform(df[columns_to_normalize])
    elif method == 'min-max':
        scaler = MinMaxScaler()
        df_normalized[columns_to_normalize] = scaler.fit_transform(df[columns_to_normalize])
    else:
        raise ValueError("Invalid normalization method. Choose 'z-score' or 'min-max'.")

    return df_normalized

# Ejemplo con tu dataset
# Suponiendo que el DataFrame se llama `df` y excluimos "Name", "Status", y "name"
normalized_df = normalize_dataframe(filtered_df, method='min-max', exclude_columns=['Name', 'Status', 'name'])
print(normalized_df.head())



# If you want to remove the column in-place (without creating a new DataFrame), use the following:
normalized_df.drop('name', axis=1, inplace=True)


normalized_df


# Save the DataFrame to an Excel file
flat_file = 'cleand_df.csv'  # Replace with your desired file path and name
normalized_df.to_csv(flat_file, index=False)
